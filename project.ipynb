{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP2wU5OtihT6"
      },
      "source": [
        "# Classification of Inappropriate/Offensive Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmFLzbUMivyU"
      },
      "source": [
        "## Import the essentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHJ4wBofhr8S"
      },
      "outputs": [],
      "source": [
        "!pip install zeyrek\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import zeyrek\n",
        "import re\n",
        "import contextlib\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Text vectorizing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from radient import text_vectorizer\n",
        "\n",
        "# Modelling\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Bernoulli for bag-of-words, Multinomial for Tf-idf and Gaussian for sentence transformers\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "\n",
        "# MLP Classifier for sentence transformers\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Tree Visualisation\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import Image\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def suppress_output():\n",
        "    with contextlib.redirect_stdout(io.StringIO()) as stdout, \\\n",
        "         contextlib.redirect_stderr(io.StringIO()) as stderr:\n",
        "        yield (stdout, stderr)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('dataset.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciDALDINikWj"
      },
      "source": [
        "## Data Preprocessing Part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE0aIKatlyOl"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcTC4QNXBUnD"
      },
      "source": [
        "***Data Cleaning***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wnFCVPjhh7S2"
      },
      "outputs": [],
      "source": [
        "\n",
        "analyzer = zeyrek.MorphAnalyzer()\n",
        "\n",
        "# Function to remove unnecessary user tags from the entries\n",
        "def clean_text(text):\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove special characters, punctuation, and numbers\n",
        "    text = re.sub(r'[^a-zA-ZçğıöşüÇĞİÖŞÜ\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('turkish'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatized_words = []\n",
        "    for word in words:\n",
        "        with suppress_output():\n",
        "            analyses = analyzer.analyze(word)\n",
        "        if analyses:\n",
        "            lemmatized_words.append(analyses[0][0].lemma)\n",
        "        else:\n",
        "            lemmatized_words.append(word)\n",
        "\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Apply the cleaning function to the 'text' column\n",
        "for i in tqdm(range(data['text'].size)):\n",
        "    data['text'].iloc[i] = clean_text(data['text'].iloc[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNVdf1ljk35B"
      },
      "source": [
        "## Exploratory Data Analysis Results\n",
        "The dataset includes 53005 rows and is about to offensive text. We can clearly see that our data has 2 columns which are not null: *text* and *label*. So we don't need to clear or fill the null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKvmc_sqjXkN"
      },
      "outputs": [],
      "source": [
        "data.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEqlhnkpeSXw"
      },
      "source": [
        "### Most used words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEO-rKNViXZI"
      },
      "outputs": [],
      "source": [
        "def find_most_common(data, number = 50):\n",
        "  words_filter = ['@', '.', 'USER', ',', '#', 've', 'bir', 'bu', 'Bu', 'de', 'ne', '!' ':', 'https', '\"', '...', '``']\n",
        "  text_list = data['text'].tolist()\n",
        "\n",
        "  # Join all the strings in the list into a single string\n",
        "  text = ' '.join(text_list)\n",
        "\n",
        "  # Tokenize the text into words\n",
        "  words = nltk.word_tokenize(text)\n",
        "  # Count the frequency of each word\n",
        "  freq_dist = FreqDist(words)\n",
        "  stopwords = nltk.corpus.stopwords.words('turkish')\n",
        "  dict_filter = lambda freq_dist, stopwords: dict( (word,freq_dist[word]) for word in freq_dist if (word not in stopwords and word not in words_filter and len(word) > 1) )\n",
        "  filtered_freq_dist = dict_filter(freq_dist, stopwords)\n",
        "\n",
        "  print(type(filtered_freq_dist))\n",
        "  # Get the most frequent words\n",
        "  most_frequent_words = FreqDist(filtered_freq_dist).most_common(number)\n",
        "  return most_frequent_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE-SbZtzeRR_"
      },
      "outputs": [],
      "source": [
        "# Filter the symbols and most common adjuncts in Turkish\n",
        "# Convert the text column to a list of strings\n",
        "data_offensive = data[data['label'] == 1]\n",
        "data_not_offensive = data[data['label'] == 0]\n",
        "\n",
        "most_frequent_words_in_offensive = find_most_common(data_offensive)\n",
        "most_frequent_words_not_in_offensive = find_most_common(data_not_offensive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcEAljUhfVpL"
      },
      "outputs": [],
      "source": [
        "# Extract the words and frequencies from the most_frequent_words list\n",
        "words, frequencies = zip(*most_frequent_words_in_offensive)\n",
        "\n",
        "# Create a bar chart of the most frequent words\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.bar(words, frequencies)\n",
        "plt.xlabel(\"Words\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Most Frequent Words in the Offensive Sentences\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Yy4LvDmuCUq"
      },
      "outputs": [],
      "source": [
        "# Extract the words and frequencies from the most_frequent_words list\n",
        "words, frequencies = zip(*most_frequent_words_not_in_offensive)\n",
        "\n",
        "# Create a bar chart of the most frequent words\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.bar(words, frequencies)\n",
        "plt.xlabel(\"Words\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Most Frequent Words in the Non-offensive Sentences\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZJtrxPHRMSq"
      },
      "source": [
        "## Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6ZLjZ1EekMb"
      },
      "outputs": [],
      "source": [
        "# Use bag-of-words method to vectorize text\n",
        "# 'bow' signifies the bag-of-words\n",
        "\n",
        "data = pd.read_csv('cleaned_dataset.csv', encoding='utf-8')\n",
        "vectorizer_bow = CountVectorizer(encoding='utf-8')\n",
        "X_bow = vectorizer_bow.fit_transform(data['text'].values.astype(str))\n",
        "\n",
        "# use bool type for each numeric to reduce size\n",
        "\n",
        "X_bow = X_bow.astype(bool).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Tf-idf vectorizer\n",
        "# 'idf' refers to the Tf-idf vectorizer\n",
        "\n",
        "vectorizer_idf = TfidfVectorizer(encoding='utf-8')\n",
        "X_idf = vectorizer_idf.fit_transform(data['text'].values.astype(str))"
      ],
      "metadata": {
        "id": "D3PtqYFicJQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use 'BGE-M3' model from sentence-transformers to vectorize raw sentences\n",
        "# The results are vectors with 1024-dimensions. 'st' signifies the sentence transformers\n",
        "\n",
        "X_st= []\n",
        "vectorizer_st = text_vectorizer(method=\"sbert\", model_name_or_path=\"BAAI/bge-m3\")\n",
        "data_raw = pd.read_csv('dataset.csv', encoding='utf-8')\n",
        "for row in tqdm(data_raw['text'], total=data_raw.shape[0]):\n",
        "    X_st.append(vectorizer_st.vectorize(str(row)))"
      ],
      "metadata": {
        "id": "fPkZYnt9cLHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the resulting datasets for training and testing\n",
        "\n",
        "\n",
        "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, data['label'].astype(bool), test_size=0.2)\n",
        "X_train_idf, X_test_idf, y_train_idf, y_test_idf = train_test_split(X_idf, data['label'].astype(bool), test_size=0.2)\n",
        "X_train_st, X_test_st, y_train_st, y_test_st = train_test_split(X_st, data['label'].astype(bool), test_size=0.2)"
      ],
      "metadata": {
        "id": "4Jf6fo-KcMbE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}